# Домашнее задание к занятию 17 «Инцидент-менеджмент»
## Основная часть
Постмортем на основе реального сбоя системы GitHub в 2018 году.

- **Краткое описание инцидента**</br>
21.10.2018 около 23:00 по UTC в работе сервисов GitHub произошел сбой. В течение 24 часов не работали веб-хуки (webhooks), было невозможно публиковать сайты GitHub Pages, создавать новые Issue и комментировать уже существующие. Инцидент не затронул пользовательские данные, Git-репозитории не пострадали.
- **Предшествующие события**</br>
21.10.2018 проводилась плановая работа по замене отказавшего 100-гигабитного сетевого оборудования в дата-центре.
- **Причина инцидента**</br>
Из-за замены сетевого оборудования на 43 секунды пропала связь между сетевым хабом и основным дата-центром, расположенными на Восточном побережье США.
Во время отсутствия связи, Orchestrator - служба, отвечающая за управление распределенным MySQL кластером и автоматическое восстановление работоспособности (automated failover), начала процесс перестроения кластерной топологии согласно протоколу Raft и назначила ответственным (primary) за операции записи кластер в дата-центре на Западном побережье.
- **Воздействие**</br>
После восстановления связи, приложения стали отправлять все операции записи на Западное побережье, однако в кластере в дата-центре Восточного побережья осталось несколько не реплицированных операций записи. В результате в каждом из кластеров на Востоке и на Западе появились записи, не реплицированные с другим кластером. Репликация оказалась нарушена, произошел split-brain. Кроме того, из-за возросшей задержки (latency) (приложения с Востока отправляли запросы на запись на Запад), часть служб перестала вовремя отвечать на пользовательские запросы.
- **Обнаружение**</br>
В 23:02 UTC дежурные инженеры обратили внимание на сообщения системы оповещения о сбоях в работе служб, а также обнаружили, что некоторые кластеры БД находятся в нештатном состоянии, в топологии репликации отсутствовали серверы дата-центра Восточного побережья. В 23:13 UTC к устранению сбоя были привлечены специалисты, отвечающие за работу БД.
- **Реакция**</br>
В первую очередь, инженеры отключили внутренние службы развертывания (deploy), чтобы предотвратить внесение дополнительных несанкционированных изменений в систему.
Во-вторых, было принято решение остановить работу webhooks и GitHub Pages, чтобы не усугублять состояние split-brain (для команды GitHub консистентность данных находится в большем приоритете, чем доступность).
На третьем этапе было принято решение восстановить данные из бэкапа, синхронизировать реплики в обоих дата-центрах, восстановить рабочую топологию и только после этого включить остановленные службы.
- **Восстановление**</br>
21.10.2018 в 00:41 UTC начался процесс восстановления данных из бэкапа. Инженеры в это время искали способы дополнительно ускорить процесс.
Так как webhook не обрабатывались, во время восстановления им увеличили TTL, чтобы избежать их потери. Во время восстановления анализировались логи MySQL для поиска нереплицированных записей. Кроме того, потребовалась дополнительная балансировка из-за возросшей нагрузки на время восстановления
- **Таймлайн**</br>
  __22.10.2018:__
  - 22:52 UTC - потеря связи из-за работ по замене сетевого оборудования, начало процесса изменения кластерной топологии
  - 22:54 UTC - в систему оповещения начали поступать сообщения о сбоях в работе служб
  - 23:02 UTC - команда дежурных инженеров обратила внимание на сбой и начала его расследование
  - 23:07 UTC - принято решение об приостановке внутренних deploy-сервисов
  - 23:09 UTC - системе присвоен "желтый" статус доступности
  - 23:11 UTC - к работе подключился координатор инцидентов (incident coordinator)
  - 23:13 UTC - системе присвоен "красный" статус доступности, к работе подключились инженеры БД
  - 23:19 UTC - остановлена работа webhook и GitHub Pages</br>
  __22.10.2018:__
  - 00:05 UTC - начало работы над планом восстановления работоспособности сервисов
  - 00:41 UTC - начало процесса восстановления данных из бэкапа
  - 06:51 UTC - процесс восстановления данных из бэкапа завершился, началась репликация данных из дата-центра на Западном побережье в дата-центр на Восточном побережье
  - 07:46 UTC - опубликован первый отчет об инциденте в блоге GitHub
  - 11:12 UTC - восстановлена работоспособность primary серверов в дата-центре на Восточном побережье
  - 13:15 UTC - принято решение о создании дополнительных read-реплик в дата-центре на Восточном побережье для компенсации возросшей нагрузки
  - 16:24 UTC - завершилась репликация данных, восстановлена исходная топология кластеров
  - 16:45 UTC - началась обработка скопившихся в очереди webhook и GitHub Pages
  - 23:03 UTC - работа webhook и GitHub Pages восстановлена, системе присвоен "зеленый" статус доступности
- **Последующие действия**</br>
  - Была изменена конфигурация Orchestrator таким образом, чтобы процесс выбора primary серверов был раздельным для каждого из кластеров в дата-центрах на Восточном и Западном побережьях
  - Ускорена миграция на новую систему отчетов об инцидентах, позволяющую предоставлять пользователям более подробную информацию о сбоях 
  - За несколько недель до сбоя началась работа по поддержке обслуживания трафика GitHub из нескольких дата-центров с резервированием по схеме N+1. Цель этой работы — сделать допустимым полный отказ одного дата-центра так, чтобы это не отразилось на пользователях. По итогам инцидента работа была ускорена.
  - Начата системная практика проверки сценариев возможных сбоев с целью их предотвращения
