# Домашнее задание к занятию "6.6. Troubleshooting"
## Задача 1

1. С помощью команды db.currentOp() находим **opid** зависшей CRUD операции. </br> 
Например:
```console
sample> db.currentOp()
{
  inprog: [
    {
      type: 'op',
      host: 'b2fb0eede135:27017',
      desc: 'JournalFlusher',
      active: true,
      currentOpTime: '2022-12-09T11:57:50.179+00:00',
      opid: 19951,
      op: 'none',
      ns: '',
      command: {},
      numYields: 0,
      locks: {},
      waitingForLock: false,
      lockStats: {},
      waitingForFlowControl: false,
      flowControlStats: {}
    },
  ],
  ok: 1
}
```

2. С помощью команды db.killOp() останавливаем зависшую CRUD операцию
```console
sample> db.killOp(19951)
{ info: 'attempting to kill op', ok: 1 }
```

Операции Read останавливаются на всем кластере. 
Операции Write, ассоциированные с серверной сессией, можно остановить путем уничтожения
сессии командой killSessions(<lsid>) (для этого нужно предварительно найти **lsid** (logical session
id) с помощью той же db.currentOp() ), а не ассоциированные с помощью db.killOp(<opid>) 
на каждом шарде кластера.

Для решения проблемы с долгими (зависающими) запросами нужно
проанализировать, почему запрос выполняется так долго.
Техническое решение заключается в задании ограничения времени выполнения запроса 
с помощью метода maxTimeMS(). Тогда MongoDB автоматически прервет операцию, 
превысившую лимит. </br> 
Например:
```console
db.location.find( { "town": { "$regex": "(Pine Lumber)",
                              "$options": 'i' } } ).maxTimeMS(30)
```

## Задача 2
Согласно документации, Redis удаляет истекшие записи 2-мя способами:
- "ленивый" способ - удаляются записи с истекшим TTL тогда, когда истекший TTL обнаружен в момент обращения к записи;
- "активный" способ - Redis удаляет записи с истекшим TTL по адаптивному алгоритму, цикл которого запускается 10 раз в секунду. Алгоритм с настройками по-умолчанию удаляет до 200 записей в секунду и стремится к тому, чтобы количество истекших и подлежащих удалению записей не превышало 25% от общей выборки в каждом его цикле. Соответственно, если в какой-то момент
истекло большое количество ключей и их количество превышает 25% в общей выборке, это становится причиной задержек, т.к. операция удаления является блокирующей.</br>

Очевидно, что это и произошло в нашем случае. При масштабировании начался рост отношения записанных значений к истекшим, т.к. для репликации тоже требуется какое-то время, зависящее от объема базы и пропускной способности сети. В какой-то момент количество истекших значений превысило 25% от общей выборки и начались задержки.


## Задача 3
Согласно документации (https://dev.mysql.com/doc/refman/8.0/en/gone-away.html) ошибка в 
большинстве случаев возникает, когда сервер закрывает соединение по таймауту. 
Судя по описанию задания, ошибка воспроизводится (не является результатом случайного 
единичного события, сбоя сети, действий администратора и т.п.) и начала возникать 
после превышения размера таблиц в базе определенного размера. 
Предполагаем, что логические ошибки в запросах отсутствуют (запросы не изменялись).
Следовательно, наиболее вероятная причина - запросы стали выполняться слишком долго на 
возросшем объеме данных. Можно попробовать увеличить таймауты: 
[wait_timeout](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_wait_timeout)
и [wait_timeout](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_connect_timeout)
Однако это временное решение. Необходимо проанализировать запросы, выявить возможные узкие места
и переписать их. Если это невозможно - попробовать партицировать таблицы. 


## Задача 4
**Что происходит?** </br>
Механизм ядра Linux OOM Killer обнаружил, что исчерпана свободная системная
память и принудительно закрыл процесс postgres.</br>

**Как решить данную проблему?** </br>
В первую очередь, убедиться, что именно postgres является
причиной oom-killer. Для этого проверить в системе мониторинга потребление памяти postgres.
Скорее всего, в какой-то момент postgres пытается зарезервировать памяти больше, чем может
предоставить операционная система. Если это так, можно попробовать выполнить тюнинг параметров
в _postgresql.conf_. Изменять в первую очередь следует параметры, влияющие на работу с памятью:
- _shared_buffers_ - объём памяти для буферов в разделяемой памяти
- _huge_pages_ - будут ли огромные страницы запрашиваться из основной области общей памяти
- _temp_buffers_ - максимальный объём памяти, выделяемой для временных буферов
- _max_prepared_transactions_ - максимальное число транзакций, которые могут одновременно находиться в 
«подготовленном» состоянии
- _work_mem_ - максимальный объём памяти, который будет использоваться во внутренних операциях при обработке
запросов
- _maintenance_work_mem_ - максимальный объём памяти для операций обслуживания БД
- _autovacuum_work_mem_ -  максимальный объём памяти для AVTOVACUUM
- _max_stack_depth_ - максимальная безопасная глубина стека
- _shared_memory_type_ - механизм разделяемой памяти
- _dynamic_shared_memory_type_ - механизм динамической разделяемой памяти

Далее, как и в случае с MySQL, следует провести оптимизацию запросов.</br>
Если ничего больше не помогает, увеличить физические ресурсы сервера.</br>